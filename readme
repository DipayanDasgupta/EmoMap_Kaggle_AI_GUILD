EmoMap Challenge - Emotion Classification from Facial Images
This repository contains the code and approach used for the EmoMap Challenge on Kaggle. The objective of this challenge is to build a machine learning model that classifies facial images into one of seven emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.

Dataset Overview
The dataset comprises 48x48 pixel grayscale images of human faces, each associated with one of seven emotion labels. The images have been aligned and resized to ensure consistent placement within the 48x48 pixel frames.

The training dataset consists of pixel values of images stored as strings, and each image corresponds to an emotion label from the following categories:

Emotion Index	Emotion
0	Angry
1	Disgust
2	Fear
3	Happy
4	Sad
5	Surprise
6	Neutral
Project Structure
train_dataset.csv: Training data containing image pixels and corresponding emotion labels.
test_dataset.csv: Test data with image pixels for which emotion labels need to be predicted.
submission.csv: The submission file generated by the model, mapping image IDs to predicted emotion labels.
Methodology
The key steps and methods used in this project include:

1. Data Preprocessing
Pixel String Conversion: The pixel values in the dataset are stored as space-separated strings. These strings are converted into 48x48 numpy arrays.
Normalization: Pixel values are normalized to range between 0 and 1 by dividing by 255.
Reshaping: The data is reshaped to add a channel dimension (1 channel for grayscale images).
2. Data Augmentation and Transformations
To enhance the model's ability to generalize, data augmentation techniques were applied using the torchvision.transforms module:

Random Horizontal Flip: Randomly flips the image horizontally.
Random Rotation: Randomly rotates the image by 10 degrees.
Normalization: Each image is normalized to have a mean of 0.5 and a standard deviation of 0.5.
3. CNN Model Architecture
A custom Convolutional Neural Network (CNN) was designed for this classification task. The architecture consists of:

Three Convolutional Layers: Each followed by batch normalization, ReLU activation, and max pooling.
Conv2D (32 filters) → Conv2D (64 filters) → Conv2D (128 filters)
Fully Connected Layers: After flattening the feature maps, two fully connected layers were used.
FC1 (128 * 6 * 6 → 256) → FC2 (256 → 7 for emotion classification)
Dropout (0.5) was applied to prevent overfitting.
4. Loss Function and Optimizer
Loss Function: Cross-entropy loss (nn.CrossEntropyLoss) is used to handle the multiclass classification problem.
Optimizer: The Adam optimizer (torch.optim.Adam) was used with a learning rate of 0.001 to update the model's weights during training.
5. Training and Evaluation
Train-Validation Split: The data was split into 80% for training and 20% for validation to monitor the model's performance.
Training Process: The model was trained for 25 epochs, during which the training loss, training accuracy, validation loss, and validation accuracy were monitored at each epoch.
Evaluation: Validation was done after each epoch to evaluate the model's generalization ability on unseen data.
6. Predictions on Test Data
After training the model, the test dataset was preprocessed, and predictions were generated using the trained model. These predictions were then saved to submission.csv for submission on Kaggle.
Results
Final Model Performance: The model achieved a strong accuracy on both the training and validation sets, with continuous monitoring of overfitting via validation loss.
Prediction File: The predictions generated by the model for the test dataset have been saved in the file submission.csv, which follows the required format of image ID and predicted emotion.
How to Use
Clone the repository:

bash
Copy code
git clone https://github.com/yourusername/emo-map-challenge.git
Install required dependencies:

bash
Copy code
pip install -r requirements.txt
Run the model: You can execute the training process by running the provided Python scripts:

bash
Copy code
python train.py
Predict on test dataset:

bash
Copy code
python predict.py
Generate submission file: After predictions are made, the submission.csv file is automatically generated.

Conclusion
This project demonstrates the effective use of a custom CNN model for emotion classification from facial images. The model was trained and evaluated using standard deep learning techniques, and predictions were generated for the EmoMap Challenge.

Feel free to explore, modify, and improve the model!

Here's how you can incorporate that into the README:

Results
Final Model Performance: The model attained an accuracy of 49% on the Kaggle test data. While this result highlights the challenge of emotion classification from grayscale images, it also demonstrates the potential of using CNNs to tackle complex visual recognition tasks.
Prediction File: The predictions generated by the model for the test dataset have been saved in the file submission.csv, which follows the required format of image ID and predicted emotion.
